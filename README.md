# Project Manise ğŸï¸

Turning static PDFs into a dynamic parallel corpus for Ambonese Malay.

Project Manise is a Python pipeline designed to rescue dictionary data from the clutches of PDF formatting. It extracts, parses, cleans, and aligns text from Ambonese Malay dictionaries, converting them into high-quality, structured JSON and parallel sentence pairs.

## The Mission:
The primary purpose of this project is to collect and structure authentic linguistic data. By converting static documentation into machine-readable formats, we create a foundational dataset that can be augmented to train more robust language models on under-represented languages.

"Manise" implies sweetness and beautyâ€”we're taking messy, raw data and making it sweet, structured, and ready for training.

## ğŸ§ What is this?

Ambonese Malay is a vibrant language, but like many regional tongues, it is under-resourced in the digital space. This project bridges the gap by:

- Ingesting raw dictionary PDFs.
- Structuring raw text into semantic components (Headwords, Definitions, Examples).
- Generating clean parallel corpora (Ambonese <-> Indonesian) for NLP tasks.

## ğŸš€ The Pipeline

We process data moving from raw extraction to structured gold:

- **ğŸ“„ Text Extraction**: Rips text from double-column PDFs (even the messy parts).
- **ğŸ§  Semantic Parsing**: Identifies linguistic categories (Headwords vs. Definitions) and structures the text into JSON.
- **ğŸ§¹ The Cleanup**: Hunts down OCR artifacts. Turns placeholder symbols into actual words and fixes spacing issues.
- **ğŸ“ Correction**: Fixes OCR errors and creates a clean text stream.
- **âœ¨ Corpus Generation**: Extracts aligned sentence pairs (Ambonese -> Indonesian) for immediate use in NLP training.

## ğŸ› ï¸ Requirements

- Python 3.7+
- LLM API Key (Set as `GEMINI_API_KEY` or `GOOGLE_API_KEY`).

## ğŸ“¦ Installation

```bash
# Clone the repo
git clone https://github.com/dansachs/project-manise.git
cd project-manise

# Install the dependencies
pip install -r requirements.txt

# Set your API Key
export GEMINI_API_KEY="your-api-key-here"
```

## ğŸ’» Usage Guide

### 1. Extract (Get the text out)

Pull raw text from the PDF, handling columns automatically.

```bash
python 1_extract_dictionary_text.py dictionary_20260105.pdf --start-page 16
```

**Output**: `outputs/extractions/extraction_TIMESTAMP.txt`

### 2. Parse (Make it structured)

Feed the raw text to the model to identify Headwords, Definitions, and Examples.

```bash
python 2_parse_dictionary_entries.py
```

**Output**: `outputs/parsed/entries_TIMESTAMP.json`

### 3. Clean (Scrub the artifacts)

Standardize placeholders and fix the "OCR jitter."

```bash
python 3_clean_placeholders.py outputs/parsed/entries_TIMESTAMP.json

# Optional: Specialized dash cleaning
python 3.1_replace_dash_space_dash.py outputs/cleaned/progress_TIMESTAMP.json
```

**Output**: `outputs/cleaned/progress_TIMESTAMP_original_cleaned.json`

### 4. Correct (Fix typos)

Corrects OCR slips while maintaining the integrity of the text.

```bash
python 4_correct_ocr_typos.py outputs/cleaned/progress_TIMESTAMP.json
```

**Output**: `outputs/corrected/corrections_TIMESTAMP.json`

### 5. Extract (Build the dataset)

Generate the final gold standard: parallel sentences for training.

```bash
python 5_extract_parallel_sentences.py outputs/cleaned/progress_TIMESTAMP.json --count 100
```

**Output**: `outputs/parallel_sentences_100.jsonl`

**The Result:**

```json
{"ambonese": "Beta pigi ka pasar", "indonesian": "Saya pergi ke pasar"}
```

## ğŸ“‚ Project Structure

```
project-manise/
â”œâ”€â”€ 1_extract_dictionary_text.py      # The Extractor
â”œâ”€â”€ 2_parse_dictionary_entries.py     # The Parser
â”œâ”€â”€ 3_clean_placeholders.py           # The Cleaner
â”œâ”€â”€ 4_correct_ocr_typos.py            # The Corrector
â”œâ”€â”€ 5_extract_parallel_sentences.py   # The Miner
â”œâ”€â”€ convert_json_csv.py               # Converter (JSON <-> CSV)
â”œâ”€â”€ utils/                            # Utilities
â”‚   â”œâ”€â”€ change_tracker.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â””â”€â”€ validators.py
â””â”€â”€ outputs/                          # Generated Data
    â””â”€â”€ parallel_sentences_100.jsonl
```

## ğŸ“Š Data Formats

### The Nested JSON (Rich Dictionary Data):

```json
{
  "headword": "pigi",
  "meanings": [
    {
      "definition": "pergi",
      "ambonese_example": "Beta pigi ka pasar",
      "indonesian_translation": "Saya pergi ke pasar"
    }
  ]
}
```

### The Flat JSONL (Ready for Training):

```json
{"ambonese": "Beta pigi ka pasar", "indonesian": "Saya pergi ke pasar"}
```

## ğŸ“ Notes

- **Resume Capability**: Scripts track progress and can resume where they left off if interrupted.
- **Logs**: Detailed logs are saved to `outputs/logs/`.
- **Cost Warning**: This pipeline utilizes API calls. Monitor usage when processing large dictionaries.

Built for the documentation and preservation of Ambonese Malay.
